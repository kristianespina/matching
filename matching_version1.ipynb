{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CCUBE MATCHING SYSTEM 1.0\n",
    "* PETER BAZANOV\n",
    "* PART1- SEMANTIC INTELLIGENCE BASED ON METRICS FOR MATCHING (FEATURE ENGINEERING)\n",
    "  - VECTOR ENCODING\n",
    "  - SIMILARITY DISTANCE & SEMANTIC\n",
    "  - TOP N DECISION\n",
    "* PART2 (IMROVEMENTS IN PROGRESS)- XGBOOST MACHINE LEARNING FOR DECISION INTELLIGENCE ON TARGET ID  (ERROR CORRECTION OF 1ST LOGIC)\n",
    "- WORK ON RECOMMENDATIONS OF TARGET ID\n",
    "- SEMI SUPERVISED MODEL\n",
    "- WORK ON ERROR SUPERVISED USER CORRECTION\n",
    "- WORK ON HiSTORY DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from mahalanobis import Mahalanobis\n",
    "%matplotlib inline\n",
    "from fastnumbers import fast_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and make sure that all of the empty strings have been filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lead id: [10002] ContaMediaAccount: [102] Distance: 0.0 Final: 0.0 Final2: 0.022360679774997897\n",
      "Lead id: [10002] ContaMediaAccount: [122] Distance: 103.44177429680256 Final: 0.0 Final2: 0.019913769751184668\n",
      "Lead id: [10012] ContaMediaAccount: [102] Distance: 55.2318323740417 Final: 0.0 Final2: 0.021089527439607514\n",
      "Lead id: [10012] ContaMediaAccount: [122] Distance: 63.937174170077725 Final: 210.0594677551169 Final2: 0.5026004737904812\n",
      "Lead id: [10022] ContaMediaAccount: [102] Distance: 103.44177429680256 Final: 0.0 Final2: 0.019913769751184668\n",
      "Lead id: [10022] ContaMediaAccount: [122] Distance: 0.0 Final: 0.0 Final2: 0.022360679774997897\n",
      "Lead id: [10032] ContaMediaAccount: [102] Distance: 2.223898532891522 Final: 0.0 Final2: 0.022310896473855743\n",
      "Lead id: [10032] ContaMediaAccount: [122] Distance: 105.26127269645991 Final: 0.0 Final2: 0.01986803279903524\n",
      "Lead id: [10042] ContaMediaAccount: [102] Distance: 54.41266271933869 Final: 0.0 Final2: 0.02110893974790447\n",
      "Lead id: [10042] ContaMediaAccount: [122] Distance: 66.14221636397575 Final: 0.0 Final2: 0.02082925307436693\n",
      "Lead id: [10052] ContaMediaAccount: [102] Distance: 101.62776813169552 Final: 0.0 Final2: 0.019959264311800288\n",
      "Lead id: [10052] ContaMediaAccount: [122] Distance: 2.223898532891522 Final: 0.0 Final2: 0.022310896473855743\n",
      "Lead id: [10062] ContaMediaAccount: [102] Distance: 5.805169650810911 Final: 126.11257789219884 Final2: 0.27741846801176373\n",
      "Lead id: [10062] ContaMediaAccount: [122] Distance: 107.10077609372831 Final: 0.0 Final2: 0.019821685697898443\n",
      "Lead id: [10072] ContaMediaAccount: [102] Distance: 54.03275263237123 Final: 0.0 Final2: 0.02111793662666002\n",
      "Lead id: [10072] ContaMediaAccount: [122] Distance: 67.2452031688444 Final: 0.0 Final2: 0.020802759356180506\n",
      "Lead id: [10082] ContaMediaAccount: [102] Distance: 99.95952213111295 Final: 0.0 Final2: 0.020001011921122566\n",
      "Lead id: [10082] ContaMediaAccount: [122] Distance: 5.8107606444436275 Final: 0.0 Final2: 0.02223036750383485\n",
      "Lead id: [10092] ContaMediaAccount: [102] Distance: 7.461010008377946 Final: 0.0 Final2: 0.022193219459817497\n",
      "Lead id: [10092] ContaMediaAccount: [122] Distance: 107.14367069871595 Final: 139.78836302908013 Final2: 0.3756462645988451\n",
      "Lead id: [10102] ContaMediaAccount: [102] Distance: 54.627685050840405 Final: 0.0 Final2: 0.021103845975299376\n",
      "Lead id: [10102] ContaMediaAccount: [122] Distance: 70.37516868858373 Final: 0.0 Final2: 0.02072739325895604\n",
      "Lead id: [10112] ContaMediaAccount: [102] Distance: 98.40386810653418 Final: 0.0 Final2: 0.020039863569731853\n",
      "Lead id: [10112] ContaMediaAccount: [122] Distance: 9.515414532329645 Final: 0.0 Final2: 0.02214688658632789\n",
      "Lead id: [10122] ContaMediaAccount: [102] Distance: 10.208965013504612 Final: 0.0 Final2: 0.022131223079317044\n",
      "Lead id: [10122] ContaMediaAccount: [122] Distance: 108.18113192119695 Final: 0.0 Final2: 0.019794415072913952\n",
      "Lead id: [10132] ContaMediaAccount: [102] Distance: 55.703981236409746 Final: 0.0 Final2: 0.02107833054972785\n",
      "Lead id: [10132] ContaMediaAccount: [122] Distance: 72.45259000006578 Final: 0.0 Final2: 0.020677219590649373\n",
      "Lead id: [10142] ContaMediaAccount: [102] Distance: 97.54899565448669 Final: 0.0 Final2: 0.020061181529150103\n",
      "Lead id: [10142] ContaMediaAccount: [122] Distance: 10.562231589728594 Final: 0.0 Final2: 0.022123240459079936\n",
      "Lead id: [10152] ContaMediaAccount: [102] Distance: 13.419439105265525 Final: 0.0 Final2: 0.022058571143542698\n",
      "Lead id: [10152] ContaMediaAccount: [122] Distance: 108.8764728986622 Final: 0.0 Final2: 0.0197768432036394\n",
      "Saved\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "from category_encoders import *\n",
    "from scipy import spatial\n",
    "from sklearn.datasets import load_boston\n",
    "from category_encoders import OneHotEncoder\n",
    "\n",
    "DEB_FLAG=1\n",
    "\n",
    "def geo_distance(origin, destination):\n",
    "\n",
    "    lat1, lon1 = origin\n",
    "    lat2, lon2 = destination\n",
    "    radius = 6371 # km\n",
    "\n",
    "    dlat = math.radians(lat2-lat1)\n",
    "    dlon = math.radians(lon2-lon1)\n",
    "\n",
    "    a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) \\\n",
    "        * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    d = radius * c\n",
    "    return d\n",
    "\n",
    "\n",
    "#weighted encoding based on sales of electric vehicle\n",
    "gender_mapping = { \"female\" : 2,\"company\":5, \"male\" : 10}\n",
    "lang_mapping = { \"french\":2, \"english\":5,\"spanish\" : 8,\"portoguese\" : 10 }\n",
    "famstatus_mapping = { \"single\" : 1,\"married\" : 6}\n",
    "profession_mapping = { \"itprofessional\" : 2,\"taxidriver\" : 20,\"hotelmanager\":6,\"retailer\":7,\"biker\" : 18,\"bankmanager\" : 13,\"hotelmanager\":17,\"retailer\":8,\"architect\":9,\"propertymanager\":10, \"tourismmanager\":11,\"agriculturemanager\":1,\"none\":-1    }\n",
    "hobby_mapping = { \"swimming\" : 3,\"cycling\" : 10,\"fitness\":7,\"cinema\":1,\"shopping\" : 7,\"diving\" : 5,\"meetupsocial\":8,\"investing\":4,\"carsocial\":12,\"garden\":6, \"none\":-1 }\n",
    "sport_mapping = { \"Martial arts\" : 7,\"Watersports\" : 2,\"cycling\":10,\"dancing\":5,\"tennis\" : 6,\"motorsport\" : 11,\"Canoeing\":9,\"football\":8,\"basketball\":4,\"none\":-1 }\n",
    "travel_distance_mapping = { \"low\" : 100,\"medium\" :500,\"high\":1000}\n",
    "age_group_mapping= { \"young\" : 20,\"midage\" :30,\"senior\":50}\n",
    "lang_group_mapping= { \"nonport\" :0,\"port_spain_group\":1}\n",
    "vehicle_life_group_mapping = { \"Family\" :3,\"Mid\":1,\"VIP\":10}\n",
    "vehiclekind_mapping = { \"BMWi3\":1,\"BMW330e\":2,\"Mercedes-BenzE300\":3,\"MitsubishiOutlander\":4,\"BMW530\":5,\"NISSAN LEAF\":6,\"JaguarPace\":7,\"Hyunday\":8,\"RenaultZOE\":9,\"TeslaModel3\":10 }\n",
    "\n",
    "\n",
    "def cmp_all( dataSetI, dataSetII):\n",
    "    \n",
    "    diflang=abs(dataSetI[0]-dataSetII[0])     \n",
    "    difage=abs(dataSetI[1]-dataSetII[1])        \n",
    "    difgender=abs(dataSetI[2]-dataSetII[2])     \n",
    "    \n",
    "    if dataSetI[0] ==dataSetII[0]:\n",
    "        lang_decision=1\n",
    "    else:\n",
    "        lang_decision=0 if (dataSetI[0]<8 or dataSetII[0]<8 ) else 0.75\n",
    "          \n",
    "    age_decision = 0 if difage>=30 else (30-difage)/30        \n",
    "         \n",
    "    if difgender>=6:\n",
    "        gender_decision=0.25\n",
    "    else:\n",
    "        gender_decision=(10-difgender)/10\n",
    "                      \n",
    "    \n",
    "    res=lang_decision*age_decision*gender_decision\n",
    "    \n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "#Open Leads\n",
    "bunch = pd.read_csv('input/leads_06.csv',dtype=str)\n",
    "df = bunch     \n",
    "\n",
    "df['gender']=df['gender'].map(gender_mapping)  \n",
    "df['lang']=df['lang'].map(lang_mapping)  \n",
    "df['famstatus']=df['famstatus'].map(famstatus_mapping)  \n",
    "df['profession']=df['profession'].map(profession_mapping)  \n",
    "df['hobby']=df['hobby'].map(hobby_mapping)  \n",
    "df['sport']=df['sport'].map(sport_mapping)  \n",
    "df['travel_distance']=df['travel_distance'].map(travel_distance_mapping)  \n",
    "df['age_group']=df['age_group'].map(age_group_mapping)  \n",
    "df['lang_group']=df['lang_group'].map(lang_group_mapping)  \n",
    "df['vehicle_life_group']=df['vehicle_life_group'].map(vehicle_life_group_mapping)  \n",
    "df['vehiclekind']=df['vehiclekind'].map(vehiclekind_mapping)  \n",
    "\n",
    "#used Mapping or One Hot encoding\n",
    "#enc = OrdinalEncoder(verbose=1, return_df=True, handle_unknown='return_nan')\n",
    "#enc.fit(X)\n",
    "#df = enc.transform(X)\n",
    "\n",
    "\n",
    "# transform the dataset\n",
    "numeric_dataset = df\n",
    "numeric_dataset_file= pd.DataFrame(numeric_dataset)\n",
    "\n",
    "numeric_dataset_file['TYPE']=bunch['TYPE']\n",
    "numeric_dataset_file['userID']=bunch['userID']\n",
    "numeric_dataset_file['UserLatitude']=bunch['UserLatitude']\n",
    "numeric_dataset_file['UserLongitude']=bunch['UserLongitude']\n",
    "numeric_dataset_file['age']=bunch['age']\n",
    "numeric_dataset_file['target_id']=bunch['target_id']\n",
    "numeric_dataset_file.to_csv('temp/leads_file_out_enc3.csv', header=True, index=False) \n",
    "\n",
    "#Open Agents\n",
    "bunch= pd.read_csv('input/agents_06.csv',dtype=str)\n",
    "#print ('****agents******')\n",
    "#X=bunch[['gender','famstatus','lang','profession','hobby', 'sport','travel_distance', 'age_group','lang_group']]\n",
    "#enc = OrdinalEncoder(verbose=1, return_df=True, handle_unknown='return_nan')\n",
    "#enc.fit(X)\n",
    "#df= enc.transform(X)\n",
    "\n",
    "df=bunch\n",
    "\n",
    "df['gender']=df['gender'].map(gender_mapping)  \n",
    "df['lang']=df['lang'].map(lang_mapping)  \n",
    "df['famstatus']=df['famstatus'].map(famstatus_mapping)  \n",
    "df['profession']=df['profession'].map(profession_mapping)  \n",
    "df['hobby']=df['hobby'].map(hobby_mapping)  \n",
    "df['sport']=df['sport'].map(sport_mapping)  \n",
    "df['travel_distance']=df['travel_distance'].map(travel_distance_mapping)  \n",
    "df['age_group']=df['age_group'].map(age_group_mapping)  \n",
    "df['lang_group']=df['lang_group'].map(lang_group_mapping)  \n",
    "\n",
    "\n",
    "numeric_dataset = df\n",
    "\n",
    "numeric_dataset_file= pd.DataFrame(numeric_dataset)\n",
    "numeric_dataset_file['TYPE']=bunch['TYPE']\n",
    "numeric_dataset_file['userID']=bunch['userID']\n",
    "numeric_dataset_file['UserLatitude']=bunch['UserLatitude']\n",
    "numeric_dataset_file['UserLongitude']=bunch['UserLongitude']\n",
    "numeric_dataset_file['age']=bunch['age']\n",
    "numeric_dataset_file['target_id']=bunch['target_id']\n",
    "numeric_dataset_file['agent_experience']=bunch['agent_experience']\n",
    "numeric_dataset_file['agent_location_group']=bunch['agent_location_group']\n",
    "numeric_dataset_file['alpha']=bunch['alpha']\n",
    "numeric_dataset_file['Exclusive']=bunch['Exclusive']\n",
    "\n",
    "#One Hot encoding /Ordinal Save\n",
    "numeric_dataset_file.to_csv('temp/ag_file_out_enc3.csv', header=True, index=False) \n",
    "\n",
    "df_leads = pd.read_csv('temp/leads_file_out_enc3.csv',dtype=str)\n",
    "df_agents = pd.read_csv('temp/ag_file_out_enc3.csv',dtype=str)\n",
    "\n",
    "df_leads.head()\n",
    "df_agents.head()\n",
    "\n",
    "\n",
    "if DEB_FLAG :\n",
    "    #DISTANCE INTELLIGENCE\n",
    "    np_df_leads=np.array(df_leads[['TYPE','userID']])\n",
    "    Subdf_lead_user =df_leads[['userID']]\n",
    "    Subdf_lead =df_leads[['lang','age','gender','famstatus','profession','hobby', 'sport', 'age_group', 'lang_group']]\n",
    "    Subdf_lead_ag =df_agents[['userID']]   \n",
    "    Subdf_ag =df_agents[['lang','age','gender','famstatus','profession','hobby', 'sport', 'age_group', 'lang_group']]\n",
    "    \n",
    "    outframe= pd.DataFrame(columns=[ 'LeadID', 'ContaMediaAccount', 'Distance', 'SemDistCosine', 'SemDistHamming','SemDistCorrel','DistVIPHamming','WeightSem','Final','Final2','LeadLong','LeadLat','AgentLong','AgentLat','StoryLead','StoryAgent','VIPLeadStory','VIPAgentStory' ])\n",
    "    outframe_res=outframe\n",
    "    \n",
    "    dataSetI_distance=df_leads[['UserLatitude','UserLongitude']]\n",
    "    dataSetI_VIPLEAD = df_leads[['vehicle_life_group','vehicle_life_group','vehicle_life_group','vehicle_life_group']]\n",
    "    dataSetII_distance=df_agents[['UserLatitude','UserLongitude']]\n",
    "    dataSetII_ALPHAAG = df_agents[['agent_experience', 'agent_location_group', 'alpha', 'Exclusive']]\n",
    "    \n",
    "    for i in range (0,len(Subdf_lead.iloc[:,0])):\n",
    "        dataSetI = np.array((Subdf_lead.iloc[i,:]))\n",
    "        dataSetI_d =np.array(dataSetI_distance.iloc[i,0:2])\n",
    "        dataSetI_u = Subdf_lead_user.iloc[i,0:1]\n",
    "        dataSetI_VIPLEAD_action=np.array(dataSetI_VIPLEAD.iloc[i,0:4])\n",
    "        \n",
    "\n",
    "        for j in range(0,len(Subdf_ag.iloc[:,0])):\n",
    "            dataSetII = np.array((Subdf_ag.iloc[j,:]))\n",
    "            dataSetII_d =np.array(dataSetI_distance.iloc[j,0:2])\n",
    "            dataSetII_u = Subdf_lead_ag.iloc[j,0:1]\n",
    "            dataSetII_ALPHAAG_action=np.array( dataSetII_ALPHAAG.iloc[j,0:4])           \n",
    "\n",
    "            dataSetI = [float(i) for i in dataSetI]\n",
    "            dataSetII = [float(i) for i in dataSetII]\n",
    "\n",
    "            dataSetI_d = [float(i) for i in dataSetI_d]\n",
    "            dataSetII_d = [float(i) for i in dataSetII_d]\n",
    "\n",
    "            dataSetI_u = [int(i) for i in dataSetI_u]\n",
    "            dataSetII_u = [int(i) for i in dataSetII_u]\n",
    "\n",
    "            \n",
    "            \n",
    "                       \n",
    "            #SEMANTIC COUSINE\n",
    "            result = 1 - spatial.distance.cosine(dataSetI, dataSetII)               \n",
    "            \n",
    "            #SEMANTIC HAMMING\n",
    "            result3= 0.1+1 - spatial.distance.hamming(dataSetI, dataSetII)    \n",
    "            #GEO\n",
    "            result2 = geo_distance(np.array(dataSetI_d), np.array(dataSetII_d))            \n",
    "            \n",
    "            #CANBERRA\n",
    "            result5=  1/(spatial.distance.canberra(dataSetI, dataSetII)+0.001)    \n",
    "            \n",
    "            #WEIGHTED SEMANTIC\n",
    "            \n",
    "            result6=cmp_all(dataSetI, dataSetII)      \n",
    "            if result6<0.5:\n",
    "                result6=0  #dropout in case of dismatches gender, age, lang            \n",
    "                \n",
    "               \n",
    "            #VIP HAMMING\n",
    "            result4= 0.1+ 1 - spatial.distance.hamming(dataSetII_ALPHAAG_action, dataSetI_VIPLEAD_action)\n",
    "            \n",
    "            final=(500-result2)*result3*math.sqrt(result6)*(0.2* result+0.3*result3+0.1*result4+0.3*result5+result6*0.3)\n",
    "            final2=result3*math.sqrt(result6)*(0.2* result+0.3*result3+0.1*result4+0.3*result5+result6*0.3)+ math.sqrt(500-result2)/1000\n",
    "                                   \n",
    "            #print('Lead id:',np.array(dataSetI_u),'ContaMediaAccount:',np.array(dataSetII_u),'Distance:', result2, 'SemDistCosine:',result, 'SemDistHamming:',result3,'SemDistCorrel:',result5,'DistVIPHamming:',result4,'WeightSem:',result6,'Final:',final, 'Final2:',final2,'LeadLong:', np.array(dataSetI_d),  'LeadLat:','AgentLong:', 'AgentLat:',np.array(dataSetII_d)  )\n",
    "            if i % 10 ==1 and j % 20 ==1 :\n",
    "                print('Lead id:',np.array(dataSetI_u),'ContaMediaAccount:',np.array(dataSetII_u),'Distance:', result2, 'Final:',final, 'Final2:',final2 )\n",
    "            \n",
    "                   \n",
    "            outframe = outframe.append({'LeadID' :int(np.array(dataSetI_u)),'ContaMediaAccount': int(np.array(dataSetII_u)), 'Distance':result2, 'SemDistCosine':result, 'SemDistHamming':result3,'SemDistCorrel':result5, 'DistVIPHamming':result4,'WeightSem': result6,'Final':final,'Final2':final2,'LeadLat':np.array(dataSetI_d[0]),'LeadLong':np.array(dataSetI_d[1]),'AgentLat':np.array(dataSetII_d[0]),'AgentLong':np.array(dataSetII_d[1]) ,  'StoryLead':dataSetI,'StoryAgent':dataSetII, 'VIPLeadStory':dataSetI_VIPLEAD_action, 'VIPAgentStory':dataSetII_ALPHAAG_action  }, ignore_index=True)\n",
    "            #outframe = outframe.append({'LeadID' :int(np.array(dataSetI_u)),'ContaMediaAccount': int(np.array(dataSetII_u)), 'Distance':result2, 'SemDistCosine':result, 'SemDistHamming':result3,'SemDistCorrel':result5, 'DistVIPHamming':result4,'Final':final,'Final2':final2,'LeadLat':np.array(dataSetI_d[0]),'LeadLong':np.array(dataSetI_d[1]),'AgentLat':np.array(dataSetII_d[0]),'AgentLong':np.array(dataSetII_d[1])  }, ignore_index=True)          \n",
    "\n",
    "    \n",
    "    outframe2=outframe\n",
    "    resultfr=outframe.sort_values(by=['Final'],ascending=False).head(5*160)\n",
    "    resultfr2=outframe2.sort_values(by=['Final2'],ascending=False).head(5*160)\n",
    "    \n",
    "    resultfr3=resultfr.sort_values(by=['LeadID','Final'],ascending=False)\n",
    "    resultfr4=resultfr2.sort_values(by=['LeadID','Final2'],ascending=False)\n",
    "  \n",
    "        \n",
    " \n",
    "    \n",
    "    \n",
    "    print('Saved')\n",
    "    resultfr3.to_csv('out/file_out_075_dist_long.csv', header=True, index=True) \n",
    "    resultfr4.to_csv('out/file_out_075_sem_long.csv', header=True, index=True) \n",
    "    \n",
    "    \n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out any target with less than 5 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MACHINE LEARNING LOGIC FOR SUPERVISED LEARNING ADJUSTMENT BASED ON TARGET COLUMNS AS FUNCTION FROM FEATURES, RULES\n",
    "#APPLIED FOR LEADS\n",
    "\n",
    "\n",
    "df=df_leads\n",
    "df = df.fillna('None')\n",
    "df = df.groupby('target_id').filter(lambda x: len(x) >= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a function to get a dictionary mapper for the factors in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqtodict(sequence, start=0):\n",
    "    #transform a sequence to a set of pairs to make into a dictionary\n",
    "    n = start\n",
    "    for elem in sequence:\n",
    "        yield elem,n\n",
    "        n += 1    \n",
    "\n",
    "def getItemDict(itemvalues):\n",
    "    itemvalues = list(itemvalues.unique())\n",
    "    itemdict = {}\n",
    "    if 'None' not in itemvalues:\n",
    "        itemvalues = ['None'] + itemvalues\n",
    "    itemdict = dict( seqtodict(itemvalues))\n",
    "    return itemdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary for each one of the string inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemdict = getItemDict(df['target_id'])\n",
    "\n",
    "agecode=getItemDict(df['age'])\n",
    "genderecode=getItemDict(df['gender'])\n",
    "famstatuscode=getItemDict(df['famstatus'])\n",
    "lang1code=getItemDict(df['lang'])\n",
    "\n",
    "professioncode=getItemDict(df['profession'])\n",
    "hobbycode=getItemDict(df['hobby'])\n",
    "\n",
    "sportcode=getItemDict(df['sport'])\n",
    "agegroupcode=getItemDict(df['age'])\n",
    "\n",
    "\n",
    "f11code=getItemDict(df['UserLongitude'])\n",
    "f12code=getItemDict(df['UserLatitude'])\n",
    "f21code=getItemDict(df['userID'])\n",
    "\n",
    "#decision assisting explainable\n",
    "f22code=getItemDict(df['age_group'])\n",
    "f23code=getItemDict(df['lang_group'])\n",
    "f24code=getItemDict(df['vehicle_life_group'])\n",
    "\n",
    "target_code=getItemDict(df['target_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map all of the factors to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCodeorNone(x,dictionary):\n",
    "    output = 0\n",
    "    try:\n",
    "        output=dictionary[x]\n",
    "    except:\n",
    "        output=dictionary['None']\n",
    "    return output\n",
    "\n",
    "agecode=getItemDict(df['age'])\n",
    "\n",
    "gendercode=getItemDict(df['gender'])\n",
    "\n",
    "famstatuscode=getItemDict(df['famstatus'])\n",
    "langcode=getItemDict(df['lang'])\n",
    "sportcode=getItemDict(df['sport'])\n",
    "\n",
    "df['age'] = df['age'].apply(lambda x: getCodeorNone(x,agecode))\n",
    "df['gender'] = df['gender'].apply(lambda x: getCodeorNone(x,genderecode))\n",
    "df['famstatus'] = df['famstatus'].apply(lambda x: getCodeorNone(x,famstatuscode))\n",
    "df['lang'] = df['lang'].apply(lambda x: getCodeorNone(x,langcode))\n",
    "df['sport'] = df['sport'].apply(lambda x: getCodeorNone(x,sportcode))\n",
    "df['profession'] = df['profession'].apply(lambda x: getCodeorNone(x,professioncode))\n",
    "df['hobby'] = df['hobby'].apply(lambda x: getCodeorNone(x,hobbycode))\n",
    "\n",
    "\n",
    "df['feature11-code'] = df['UserLongitude'].apply(lambda x: getCodeorNone(x,f11code))\n",
    "df['feature12-code'] = df['UserLatitude'].apply(lambda x: getCodeorNone(x,f12code))\n",
    "df['feature21-code'] = df['userID'].apply(lambda x: getCodeorNone(x,f21code))\n",
    "df['feature22-code'] = df['age_group'].apply(lambda x: getCodeorNone(x,f22code))\n",
    "df['feature23-code'] = df['lang_group'].apply(lambda x: getCodeorNone(x,f23code))\n",
    "df['feature24-code'] = df['vehicle_life_group'].apply(lambda x: getCodeorNone(x,f24code))\n",
    "\n",
    "df['target_code'] = df['target_id'].apply(lambda x: getCodeorNone(x,itemdict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test/train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=23,stratify=df['target_id'])\n",
    "\n",
    "featurecolumns = ['age','gender','lang',\n",
    "                      \n",
    "                   'feature11-code',\n",
    "                  'feature12-code',\n",
    "                      \n",
    "     'sport','famstatus',    \n",
    "'feature22-code',\n",
    "                  \n",
    "                  'feature23-code',\n",
    "                      'feature24-code'\n",
    "                      \n",
    "           \n",
    "                  \n",
    "\n",
    "            ]\n",
    "'''\n",
    "\n",
    "\n",
    "           \n",
    "                    \n",
    "                      \n",
    " '''                       \n",
    "features_train = train[featurecolumns].values\n",
    "labels_train = train['target_code'].values\n",
    "features_test = test[featurecolumns].values\n",
    "labels_test = test['target_code'].values\n",
    "\n",
    "#print (features_train)\n",
    "#print (features_test)\n",
    "\n",
    "dtrain = xgb.DMatrix(features_train, label=labels_train)\n",
    "dtest = xgb.DMatrix(features_test, label=labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defined evaluation function, return a pair metric_name, result\n",
    "# This function takes the top 10 predictions and checks to see if the target label is in that set.\n",
    "# The error is 1 - the fraction of rows where the label is in the top 10.\n",
    "def evalerror(preds, dtrain,topNvalue=10):\n",
    "    labels = dtrain.get_label()\n",
    "    vals = np.argpartition(preds,-topNvalue)[:,-topNvalue:]\n",
    "    error = 1 - float(vals.size - np.count_nonzero((vals.transpose() - labels).transpose()))/len(labels)\n",
    "    # return a pair metric_name, result\n",
    "    return 'error', error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:0.842105\ttrain-merror:0.708333\teval-error:0.210526\ttrain-error:0.083333\n",
      "Multiple eval metrics have been passed: 'train-error' will be used for early stopping.\n",
      "\n",
      "Will train until train-error hasn't improved in 30 rounds.\n",
      "[1]\teval-merror:0.947368\ttrain-merror:0.708333\teval-error:0.210526\ttrain-error:0.041667\n",
      "[2]\teval-merror:0.894737\ttrain-merror:0.694444\teval-error:0.315789\ttrain-error:0.055556\n",
      "[3]\teval-merror:0.894737\ttrain-merror:0.708333\teval-error:0.263158\ttrain-error:0.055556\n",
      "[4]\teval-merror:0.894737\ttrain-merror:0.694444\teval-error:0.263158\ttrain-error:0.055556\n",
      "[5]\teval-merror:0.894737\ttrain-merror:0.708333\teval-error:0.263158\ttrain-error:0.069444\n",
      "[6]\teval-merror:0.894737\ttrain-merror:0.708333\teval-error:0.210526\ttrain-error:0.125\n",
      "[7]\teval-merror:0.894737\ttrain-merror:0.722222\teval-error:0.210526\ttrain-error:0.125\n",
      "[8]\teval-merror:0.894737\ttrain-merror:0.736111\teval-error:0.210526\ttrain-error:0.138889\n",
      "[9]\teval-merror:0.894737\ttrain-merror:0.736111\teval-error:0.105263\ttrain-error:0.138889\n",
      "[10]\teval-merror:0.894737\ttrain-merror:0.736111\teval-error:0.157895\ttrain-error:0.083333\n",
      "[11]\teval-merror:0.894737\ttrain-merror:0.736111\teval-error:0.157895\ttrain-error:0.166667\n",
      "[12]\teval-merror:0.894737\ttrain-merror:0.736111\teval-error:0.210526\ttrain-error:0.152778\n",
      "[13]\teval-merror:0.894737\ttrain-merror:0.736111\teval-error:0.105263\ttrain-error:0.152778\n",
      "[14]\teval-merror:0.894737\ttrain-merror:0.736111\teval-error:0.157895\ttrain-error:0.194444\n",
      "[15]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.157895\ttrain-error:0.166667\n",
      "[16]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.157895\ttrain-error:0.180556\n",
      "[17]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.105263\ttrain-error:0.111111\n",
      "[18]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.157895\ttrain-error:0.138889\n",
      "[19]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.210526\ttrain-error:0.166667\n",
      "[20]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.157895\ttrain-error:0.111111\n",
      "[21]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.157895\ttrain-error:0.180556\n",
      "[22]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.157895\ttrain-error:0.180556\n",
      "[23]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.210526\ttrain-error:0.166667\n",
      "[24]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.157895\ttrain-error:0.166667\n",
      "[25]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.210526\ttrain-error:0.166667\n",
      "[26]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.105263\ttrain-error:0.138889\n",
      "[27]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.157895\ttrain-error:0.111111\n",
      "[28]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.105263\ttrain-error:0.152778\n",
      "[29]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.105263\ttrain-error:0.125\n",
      "[30]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.052632\ttrain-error:0.138889\n",
      "[31]\teval-merror:0.842105\ttrain-merror:0.75\teval-error:0.157895\ttrain-error:0.166667\n",
      "Stopping. Best iteration:\n",
      "[1]\teval-merror:0.947368\ttrain-merror:0.708333\teval-error:0.210526\ttrain-error:0.041667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Train Model\n",
    "num_round=300\n",
    "param = {'max_depth': 5, \n",
    "    'eta': 0.2, \n",
    "    'silent': 0, \n",
    "    'gamma':2,\n",
    "    'objective':'multi:softprob',\n",
    "    'num_class':len(np.unique(labels_train))+1,\n",
    "    'seed':32}\n",
    "watchlist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "bst = xgb.train(param, dtrain, num_round, watchlist, feval=evalerror, early_stopping_rounds=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bst, open(\"xgb-python.model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions on the test set and get the \"Traditional\" accuracy - how likely was the top prediction to match the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.05263157894736842\n"
     ]
    }
   ],
   "source": [
    "preds = bst.predict(dtest, ntree_limit=bst.best_ntree_limit)\n",
    "test_labels = dtest.get_label()\n",
    "test_labels[:10]\n",
    "toppreds = np.argmax(preds,axis=1)\n",
    "print(\"Accuracy: {}\".format(accuracy_score(test_labels,toppreds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check a random sample to double-check the top N prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing random entry:\n",
      "n\n",
      "14\n",
      "a\n",
      "[0.05792449 0.06198056 0.06331748 0.06459703 0.06460196 0.06800327\n",
      " 0.07338604 0.07066616 0.07338604 0.05973689 0.06800327 0.06800327\n",
      " 0.07066616 0.06772409 0.06800327]\n",
      "[[14  8  6  5 12  3  7]\n",
      " [ 5 12  7  9  6 13  8]\n",
      " [ 5 12  7  8  6 13  2]\n",
      " [ 5 12  7  8  6  2  3]\n",
      " [14  8  1  6 12  5  7]\n",
      " [10 14  8  6 12  5  7]\n",
      " [ 5 12  7  9  6  4  8]\n",
      " [ 5 12  7  1  6  3  8]\n",
      " [12  6  9  2  8 13  3]\n",
      " [ 5 12  7  8  6 13  3]\n",
      " [14  8  6  5 12  3  7]\n",
      " [14  8  6  5 12  3  7]\n",
      " [12  7  6  2  8 13  3]\n",
      " [ 5  1 12  7  6 13  8]\n",
      " [10 14  8  6 12  5  7]\n",
      " [ 5 12  7  9  6  4  8]\n",
      " [12  7  6  2  8 13  3]\n",
      " [14  9  8  6 12  5  7]\n",
      " [ 5 12  7  9  6  2  8]]\n",
      "Actual: 4.0\n",
      "Top: 6\n",
      "Top 7 Predictions: [10 14  8  6 12  5  7]\n",
      "In-sample accuracy: 0.5789473684210527\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing random entry:\")\n",
    "topNvalue =7\n",
    "\n",
    "n = np.random.choice(len(test_labels))\n",
    "print('n')\n",
    "print(n)\n",
    "a = preds[n,:]\n",
    "print('a')\n",
    "print(a)\n",
    "topN=np.argpartition(preds,-topNvalue)[:,-topNvalue:]\n",
    "print(topN)\n",
    "print(\"Actual: {}\".format(test_labels[n]))\n",
    "print(\"Top: {}\".format(toppreds[n]))\n",
    "print(\"Top 7 Predictions: {}\".format(topN[n,]))\n",
    "\n",
    "# Get the accuracy in the top N results\n",
    "acc=float(topN.size - np.count_nonzero((topN.transpose() - test_labels).transpose()))/len(test_labels)\n",
    "print(\"In-sample accuracy: {}\".format(acc))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### score_eval = pd.DataFrame({'topN':range(1,20)})\n",
    "score_eval['error'] = score_eval['topN'].apply(lambda x: evalerror(preds,dtest,x)[1])\n",
    "score_eval.plot(x='topN',y='error',kind='scatter')\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the reload and scoring functions work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7894736842105263"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst2 = pickle.load(open(\"xgb-python.model\", \"rb\"))\n",
    "preds2 = bst2.predict(dtest, ntree_limit=bst2.best_ntree_limit)\n",
    "1-evalerror(preds2,dtest)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
